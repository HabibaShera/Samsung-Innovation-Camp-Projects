{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import tkinter.messagebox as msgbox\n",
    "import tkinter.font as font\n",
    "from tkinter import filedialog\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraping():\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        \n",
    "        title = []\n",
    "        location = []\n",
    "        company_name = []\n",
    "        job_req = []\n",
    "        job_des = []\n",
    "\n",
    "        req = requests.get(url)\n",
    "\n",
    "        if str(req) == '<Response [200]>':\n",
    "            soup = bs(req.content, 'html.parser')\n",
    "            lst = soup.find_all('a', {'class':'base-card__full-link'})\n",
    "\n",
    "            links = []\n",
    "            for i in lst:\n",
    "                links.append(i['href'])\n",
    "\n",
    "            for i in range(len(links)):\n",
    "                test_req = requests.get(links[i])\n",
    "                if str(test_req) == '<Response [200]>':\n",
    "                    soup_ = bs(test_req.content, 'html.parser')\n",
    "\n",
    "                    #title\n",
    "                    title.append(soup_.find_all('h1')[0].text)\n",
    "\n",
    "                    #location\n",
    "                    location.append(soup_.find_all('span', class_='sub-nav-cta__meta-text')[0].text)\n",
    "\n",
    "                    #company name\n",
    "                    company_name.append(soup_.find_all('span')[6].find('a').text.strip())\n",
    "\n",
    "                    #job requriments\n",
    "                    job_req.append([i.text for i in soup_.find_all('ul')[5].find_all('li')])\n",
    "\n",
    "                    #job description\n",
    "                    job_des.append([i.text for i in soup_.find_all('ul')[4].find_all('li')])\n",
    "\n",
    "\n",
    "        df = pd.DataFrame({'title':title, 'company_name':company_name, 'job_description':job_des,\n",
    "                 'job_requirments':job_req, 'location':location})\n",
    "\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def create_files(df):\n",
    "        for i in range(len(df)):\n",
    "            ind = df.iloc[i, :].index\n",
    "            value = df.iloc[i, :].values\n",
    "            dic = {x:y  for x, y in zip(ind, value)}\n",
    "            tit = df['title'][i]+'.txt'\n",
    "            tit = tit.replace('/','').replace('|', '')\n",
    "\n",
    "            with open(tit, 'w', encoding='utf-8') as f:\n",
    "                for col in ind:\n",
    "                    f.writelines(col+'\\n'+ dic[col]+'\\n\\n') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Card:\n",
    "    def __init__(self, suit, value):\n",
    "        self.suit = suit\n",
    "        self.value = value\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \" of \".join((self.value, self.suit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = tk.Tk()\n",
    "window.title('Scraping')\n",
    "        \n",
    "window.geometry(\"1000x700\")\n",
    "        \n",
    "labelFont = font.Font(family='Courier', size=15)\n",
    "t1 = tk.Label(text=\"Enter URL you want to scarp\", fg='red', font=labelFont)\n",
    "t1.pack(padx=20, pady=20)\n",
    "        \n",
    "entry = tk.Entry()\n",
    "entry.pack()\n",
    "        \n",
    "def submitValues():\n",
    "    url = entry.get() \n",
    "    card = Card(url, 'A5')\n",
    "#     data = Scraping(url)\n",
    "#     data.to_csv('Try.csv', index=False)\n",
    "    tk.Label(text='Done sucussfully.You can check the current directory, you will find the data file\\nWe will load analysis later').pack()\n",
    "    \n",
    "    options = {}\n",
    "    options['filetypes'] = [\n",
    "                    ('csv files', '.csv')\n",
    "                        ]\n",
    "    options['title'] = 'Choose a file'\n",
    "\n",
    "    filedialog.askopenfile(**options)\n",
    "\n",
    "        \n",
    "submit = tk.Button(text=\"Submit\", command=submitValues)\n",
    "submit.pack()\n",
    "\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/jobs/search/?keywords=data%20science%20jobs&location=&refresh=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
